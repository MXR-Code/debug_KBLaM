import argparse
import json
import logging
import os
import pathlib
import re
from functools import partial
from itertools import chain
from typing import Callable, Dict, List, Optional

import wandb
import numpy as np
import torch
from torch.nn.parallel import DistributedDataParallel

import transformers
from transformers import AutoTokenizer

from rich.console import Console
from rich.logging import RichHandler
from rich.progress import BarColumn, Progress, SpinnerColumn, TaskProgressColumn, TextColumn, TimeRemainingColumn
from rich.theme import Theme
from torch.nn import CrossEntropyLoss

from accelerate import Accelerator

from model_kblam_kb_encoder import KBEncoder  # å¯¼å…¥çŸ¥è¯†åº“ç¼–ç å™¨æ¨¡å‹
from model_kblam_config import KBLaMConfig  # å¯¼å…¥KBLaMé…ç½®
from model_llama3 import KblamLlamaForCausalLM  # å¯¼å…¥Llamaç”Ÿæˆæ¨¡å‹
from model_phi3 import KBLaMPhi3ForCausalLM  # å¯¼å…¥Phiæ¨¡å‹

from utils_data import augment_row, generate_multi_entity_qa, get_i_dont_know_ans  # å¯¼å…¥æ•°æ®å¤„ç†å·¥å…·
from utils_train import context_set_size_scheduler, get_kb_embd, setup_scheduler_and_optimizer  # å¯¼å…¥è®­ç»ƒå·¥å…·

from model_kblam_kb_retriever import KBRetriever  # å¯¼å…¥çŸ¥è¯†åº“æ£€ç´¢å™¨
from experiment_train_function_for_llama import *  # å¯¼å…¥Llamaè®­ç»ƒå‡½æ•°
from experiment_train_function_for_phi import *  # å¯¼å…¥Phiè®­ç»ƒå‡½æ•°

from experiment_train_config import parser  # å¯¼å…¥è®­ç»ƒé…ç½®è§£æå™¨


def create_custom_progress_bar(console: Console = None,  # type: ignore
                               color: str = "cyan",
                               show_time: bool = True,
                               show_spinner: bool = True,
                               spinner_style: str = "dots",
                               disable=False,
                               ) -> Progress:
    """
    ä½¿ç”¨Richåˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„è¿›åº¦æ¡ï¼Œé€‰æ‹©æ€§åœ°åŒ…æ‹¬æŸå¤±æŠ¥å‘Šã€‚
    :param description: ä»»åŠ¡æè¿°
    :param total: æ€»æ­¥éª¤æ•°
    :param console: Richæ§åˆ¶å°å¯¹è±¡ï¼ˆå¦‚æœä¸ºNoneï¼Œå°†åˆ›å»ºä¸€ä¸ªæ–°çš„ï¼‰
    :param color: è¿›åº¦æ¡é¢œè‰²
    :param show_time: æ˜¯å¦æ˜¾ç¤ºå‰©ä½™æ—¶é—´
    :param show_spinner: æ˜¯å¦æ˜¾ç¤ºæ—‹è½¬å›¾æ ‡
    :param spinner_style: æ—‹è½¬å›¾æ ‡æ ·å¼ï¼ˆä¾‹å¦‚ï¼Œ"dots"ï¼Œ"dots12"ï¼Œ"line"ï¼Œ"arrow"ï¼‰
    :param show_loss: æ˜¯å¦æ˜¾ç¤ºæŸå¤±ä¿¡æ¯
    :return: ä¸€ä¸ªRichè¿›åº¦å¯¹è±¡å’Œä»»åŠ¡ID
    """
    if console is None:
        console = Console()
    columns = []

    if show_spinner:
        columns.append(SpinnerColumn(spinner_name=spinner_style, style=color))

    columns.extend([
        TextColumn("[bold blue]{task.description}", justify="right"),
        BarColumn(bar_width=None, style=color, complete_style=f"bold {color}"),
        TaskProgressColumn(),
        TextColumn("[bold yellow]Loss: {task.fields[loss]:.4f}", justify="right"),
    ]
    )

    if show_time:
        columns.append(TimeRemainingColumn())

    progress = Progress(*columns, console=console, expand=True, disable=disable)  # åˆ›å»ºè¿›åº¦æ¡å¯¹è±¡
    return progress


def get_batch(qa_format_func: Callable[[str, str], str],  # è·å–ä¸€ä¸ªbatchæ•°æ®
              label_func: Callable[[torch.Tensor, List, Callable], torch.Tensor],
              dataset: List[Dict],  # æ•´ä¸ªæ•°æ®é›†
              tokenizer,
              device: torch.device,
              B: int = 20,  # é»˜è®¤batchå¤§å°ä¸º20
              random_sample=True,  # æ˜¯å¦éšæœºæŠ½æ ·
              use_data_aug=False,  # æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º
              include_outlier=False,  # æ˜¯å¦åŒ…å«å¼‚å¸¸å€¼
              multi_entities=None,  # æ˜¯å¦ä½¿ç”¨å¤šé‡å®ä½“
              use_extended_qa=False,  # æ˜¯å¦ä½¿ç”¨æ‰©å±•QA
              ):
    """
    dataset: å­—å…¸åˆ—è¡¨ï¼Œè¡¨ç¤ºçŸ¥è¯†åº“ï¼Œç”¨äºæå–QAå¯¹
    model: LLMï¼Œç”¨äºæä¾›åµŒå…¥
    kb_embedding: KBåµŒå…¥ï¼ˆå¯å¾®åˆ†ï¼‰
    B: æ‰¹å¤„ç†å¤§å°
    include_outlier: ç”Ÿæˆä¸€ä¸ªæ²¡æœ‰ç­”æ¡ˆçš„batch
    multi_entities: åˆ›å»ºæ¶‰åŠå¤šä¸ªå®ä½“çš„æ‰¹é—®
    """
    labels = []  # å­˜å‚¨æ ‡ç­¾
    if multi_entities is not None:
        assert not include_outlier  # å¦‚æœå¤šé‡å®ä½“ä¸ä¸ºNoneï¼Œåˆ™ä¸èƒ½åŒ…å«å¼‚å¸¸å€¼

    if random_sample:  # å¦‚æœéœ€è¦éšæœºæŠ½æ ·
        if multi_entities is not None:
            batch_indices = np.random.choice(len(dataset), (B, multi_entities), replace=False)  # æŠ½å–å¤šé‡å®ä½“çš„ç´¢å¼•
        else:
            batch_indices = np.random.choice(len(dataset), B, replace=False)  # æŠ½å–æ™®é€šæ•°æ®çš„ç´¢å¼•
    else:
        batch_indices = np.arange(B)  # ä¸éšæœºï¼Œè¿”å›ä¸€ä¸ªèŒƒå›´

    def get_question_and_answer(idx: int) -> tuple[str, str]:  # æ ¹æ®ç´¢å¼•è·å–é—®ç­”å¯¹
        if use_extended_qa:
            Q, A = dataset[idx]["extended_Q"], dataset[idx]["extended_A"]  # æ‰©å±•é—®ç­”

        elif multi_entities is not None:  # å¦‚æœæ˜¯å¤šé‡å®ä½“
            Q, A = generate_multi_entity_qa([dataset[i]["name"] for i in idx],
                                            [dataset[i]["description_type"] for i in idx],
                                            [dataset[i]["description"] for i in idx],
                                            )
        else:
            Q = augment_row(dataset[idx]) if use_data_aug else dataset[idx]["Q"]  # æ•°æ®å¢å¼ºæˆ–ç›´æ¥è·å–é—®é¢˜
            A = get_i_dont_know_ans() if include_outlier else dataset[idx]["A"]  # å¼‚å¸¸å€¼æˆ–ç›´æ¥è·å–ç­”æ¡ˆ
        return Q, A

    with torch.autograd.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        input_strs = []  # å­˜å‚¨è¾“å…¥å­—ç¬¦ä¸²
        real_batch_indices = []  # å­˜å‚¨å®é™…çš„batchç´¢å¼•
        for idx in batch_indices:
            Q, A = get_question_and_answer(idx)
            if Q is not None and A is not None:  # ç¡®ä¿é—®é¢˜å’Œç­”æ¡ˆéƒ½å­˜åœ¨
                input_strs.append(qa_format_func(Q, A))  # æ ¼å¼åŒ–é—®ç­”å¯¹
                real_batch_indices.append(idx)  # è®°å½•æœ‰æ•ˆçš„ç´¢å¼•
            else:
                print("Q or Answer is none")  # å¦‚æœé—®é¢˜æˆ–ç­”æ¡ˆä¸ºç©ºï¼Œæ‰“å°è­¦å‘Š
        batch_indices = real_batch_indices
        tokenizer_output = tokenizer(input_strs, return_tensors="pt", padding=True).to(device)  # ä½¿ç”¨tokenizerå¤„ç†è¾“å…¥å¹¶å‘é€åˆ°è®¾å¤‡
        input_ids, attention_masks = (tokenizer_output["input_ids"],
                                      tokenizer_output["attention_mask"],
                                      )

        labels = label_func(input_ids, input_strs, tokenizer)  # ç”¨äºæ ‡ç­¾å¤„ç†
    if include_outlier:
        # ç”Ÿæˆæ–°çš„ç´¢å¼•é›†ï¼Œç¡®ä¿çŸ¥è¯†åº“ä¸åŒ…å«é—®é¢˜æ¥æºçš„å®ä½“
        batch_indices = np.random.choice(len(dataset), B, replace=False)  # é‡æ–°ç”Ÿæˆéšæœºç´¢å¼•
    return input_ids, attention_masks, labels, batch_indices  # è¿”å›è¾“å…¥IDã€æ³¨æ„åŠ›æ©ç ã€æ ‡ç­¾å’Œbatchç´¢å¼•


def get_prefix_str(args):  # ç”Ÿæˆå®éªŒå‰ç¼€å­—ç¬¦ä¸²
    use_data_aug = args.use_data_augment
    sep_query_head = args.separate_query_head
    kb_size = args.kb_size
    dynamic_kb_size = args.dynamic_kb_size

    if dynamic_kb_size is not None:
        kb_size = "dynamic"  # éšæœºå¤§å°

    duplicate_true_kb = args.duplicate_true_kb
    length_invariance = args.length_invariance
    outlier_ratio = args.outlier_num
    use_outlier = outlier_ratio != -1  # ç¡®å®šæ˜¯å¦ä½¿ç”¨å¼‚å¸¸å€¼
    multi_entities = args.multi_entity
    use_extended_qa = args.use_extended_qa
    kb_token_layer_frequency = args.kb_token_layer_frequency
    lr = args.lr

    prefix_string = f"stage1_lr_{lr}"  # å‰ç¼€å­—ç¬¦ä¸²åŒ…å«å­¦ä¹ ç‡
    if kb_token_layer_frequency is not None:
        prefix_string += f"KBTokenLayerFreq{kb_token_layer_frequency}"
    if use_extended_qa:
        prefix_string += "UseExtendedQA"
    if multi_entities is not None:
        prefix_string += f"MultiEntities{multi_entities}"
    if use_outlier:
        prefix_string += f"UseOutlier{outlier_ratio}"
    if length_invariance:
        prefix_string += "LengthInvariant"
    if not duplicate_true_kb:
        prefix_string += "NoDuplicate"
    if kb_size is not None:
        prefix_string += f"KBSize{kb_size}"
    if sep_query_head:
        prefix_string += "SepQueryHead"
    if use_data_aug:
        prefix_string += "UseDataAug"
    return prefix_string  # è¿”å›ç”Ÿæˆçš„å‰ç¼€å­—ç¬¦ä¸²


def load_cached_embeddings(encoder_model_spec: str, dataset_dir: str, dataset_name: str, key_embd_src: str):
    # åŠ è½½ç¼“å­˜çš„åµŒå…¥
    if encoder_model_spec == "OAI":
        encoder_model_spec_str = "oai"
    else:
        encoder_model_spec_str = encoder_model_spec
    key_embds = np.load(os.path.join(dataset_dir,
                                     f"{dataset_name}_{encoder_model_spec_str}_embd_{key_embd_src}.npy",
                                     )
                        ).astype("float32")  # åŠ è½½é”®åµŒå…¥
    if key_embd_src == "answer":
        # å¦‚æœæˆ‘ä»¬ä½¿ç”¨ç­”æ¡ˆå­—ç¬¦ä¸²ä½œä¸ºé”®ï¼Œåˆ™ä¹Ÿä½¿ç”¨å®ƒä½œä¸ºå€¼å­—ç¬¦ä¸²
        value_embds = np.load(os.path.join(dataset_dir,
                                           f"{dataset_name}_{encoder_model_spec_str}_embd_answer.npy",
                                           )
                              ).astype("float32")  # åŠ è½½å€¼åµŒå…¥
    else:
        value_embds = np.load(os.path.join(dataset_dir,
                                           f"{dataset_name}_{encoder_model_spec_str}_embd_value.npy",
                                           )
                              ).astype("float32")  # åŠ è½½å¤‡é€‰å€¼åµŒå…¥
    return key_embds, value_embds  # è¿”å›é”®åµŒå…¥å’Œå€¼åµŒå…¥


def get_step_config(current_accum_step: int,
                    total_accum_step: int,
                    use_data_aug: bool,
                    outlier_num: int,
                    multi_entities: int | None,
                    use_extended_qa: bool,
                    ):
    """
    æˆ‘ä»¬çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ç”±ä¸åŒç±»å‹çš„æŒ‡ä»¤ç»„æˆã€‚
    ç­–ç•¥ï¼š
    å¼‚å¸¸QAéœ€è¦æœ€å`outlier_num`ç´¯ç§¯æ­¥éª¤ï¼›
    å¤šé‡å®ä½“QAï¼ˆå¦‚æœåŒ…å«ï¼‰å å…¶ä½™ç´¯ç§¯æ­¥éª¤çš„1/3ï¼›
    æ‰©å±•QAï¼ˆå¦‚æœåŒ…å«ï¼‰å å…¶ä½™ç´¯ç§¯æ­¥éª¤çš„1/3ï¼›
    æ ‡å‡†QAå å…¶ä½™éƒ¨åˆ†ã€‚
    """
    config = {}
    config["use_data_aug"] = use_data_aug
    config["include_outlier"] = False
    config["multi_entities"] = None
    config["use_extended_qa"] = False
    include_outlier = current_accum_step >= total_accum_step - 1 - outlier_num  # å†³å®šæ˜¯å¦åŒ…å«å¼‚å¸¸å€¼

    # å¦‚æœè¾¾åˆ°æ—¶é—´ï¼Œåˆ™å†³å®šæ˜¯å¦åŒ…å«å¼‚å¸¸å€¼
    if include_outlier:
        config["include_outlier"] = True
        return config

    if current_accum_step % 3 == 0:  # æ¯3æ­¥ä¸€æ¬¡ï¼Œå¤„ç†å¤šé‡å®ä½“
        config["multi_entities"] = multi_entities
        return config

    if current_accum_step % 3 == 1:  # å¦‚æœæ˜¯ç¬¬1æ­¥ï¼Œä½¿ç”¨æ‰©å±•QA
        config["use_extended_qa"] = use_extended_qa
        return config

    return config


def get_parameter_count(encoder):  # è·å–æ¨¡å‹å‚æ•°æ•°é‡
    param_count = 0.0
    for p in encoder.parameters():
        if p.requires_grad:  # ä»…è®¡ç®—éœ€è¦æ¢¯åº¦çš„å‚æ•°
            param_count += p.numel()  # ç´¯åŠ å‚æ•°æ•°é‡
    return param_count  # è¿”å›å‚æ•°æ€»æ•°


class Trainer:
    def __init__(self,
                 llm_model: KBLaMPhi3ForCausalLM | KblamLlamaForCausalLM,  # LLMæ¨¡å‹
                 kbretriever: KBRetriever,  # çŸ¥è¯†åº“æ£€ç´¢ç±»
                 tokenizer: transformers.PreTrainedTokenizer,  # æ ‡è®°åŒ–å™¨
                 kb_token_layer_frequency: int,  # çŸ¥è¯†åº“ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é »åº¦
                 num_steps: int,  # æ€»æ­¥æ•°
                 lr: float,  # å­¦ä¹ ç‡
                 device: torch.device | None,  # è®¾å¤‡
                 use_lr_decay: bool,  # æ˜¯å¦ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡
                 kb_size: int | List[int],  # çŸ¥è¯†åº“å¤§å°
                 llm_savename: str,  # LLMä¿å­˜çš„åç§°
                 output_dir: str,  # è¾“å‡ºç›®å½•
                 sep_query_head: bool = False,  # æ˜¯å¦åˆ†ç¦»æŸ¥è¯¢å¤´
                 max_seq_len: int | None = None,  # æœ€å¤§åºåˆ—é•¿åº¦
                 ):
        self.accelerator = Accelerator()  # åˆ›å»ºåŠ é€Ÿå™¨å¯¹è±¡
        self.logger = logging.getLogger("training")  # è·å–è®­ç»ƒè®°å½•å™¨
        self.tokenizer = tokenizer  # ä¿å­˜tokenizer
        self.sep_query_head = sep_query_head  # ä¿å­˜æ˜¯å¦åˆ†ç¦»æŸ¥è¯¢å¤´
        self.kb_token_layer_frequency = kb_token_layer_frequency  # ä¿å­˜çŸ¥è¯†åº“tokenå±‚é¢‘ç‡
        self.num_steps = num_steps  # ä¿å­˜æ€»æ­¥æ•°
        self.lr = lr  # ä¿å­˜å­¦ä¹ ç‡
        self.max_seq_len = max_seq_len  # ä¿å­˜æœ€å¤§åºåˆ—é•¿åº¦

        self.model = llm_model  # ä¿å­˜LLMæ¨¡å‹
        self.model.gradient_checkpointing_enable()  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹

        self.device = device if device is not None else self.accelerator.device  # è®¾ç½®è®¾å¤‡
        self.kbretriever = kbretriever  # ä¿å­˜KBRetrieverå¯¹è±¡
        self.kb_size = kb_size  # ä¿å­˜çŸ¥è¯†åº“å¤§å°
        self.use_lr_decay = use_lr_decay  # ä¿å­˜æ˜¯å¦ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡
        self.llm_savename = llm_savename  # ä¿å­˜LLMåç§°
        self.output_path = pathlib.Path(output_dir)  # ä¿å­˜è¾“å‡ºè·¯å¾„

        if isinstance(llm_model, KBLaMPhi3ForCausalLM):  # å¦‚æœæ˜¯Phi3æ¨¡å‹
            self._get_batch = partial(get_batch, format_QA_phi3, create_labels_for_phi3)  # è®¾ç½®batchè·å–å‡½æ•°
            self._get_params = get_phi3_query_head_parameters  # è®¾ç½®å‚æ•°è·å–å‡½æ•°
        elif isinstance(llm_model, KblamLlamaForCausalLM):  # å¦‚æœæ˜¯Llamaæ¨¡å‹
            self._get_batch = partial(get_batch, format_QA_llama, create_labels_for_llama)  # è®¾ç½®batchè·å–å‡½æ•°
            self._get_params = get_llama3_query_head_parameters  # è®¾ç½®å‚æ•°è·å–å‡½æ•°
        else:
            raise ValueError(f"{llm_model} not recognised")  # æŠ›å‡ºé”™è¯¯ï¼Œæ¨¡å‹æœªè¯†åˆ«

        self.scheduler, self.optim = self.setup_scheduler_and_optim()  # è®¾ç½®è°ƒåº¦å™¨å’Œä¼˜åŒ–å™¨

        self.model, self.optim, self._get_batch, self.kbretriever.encoder = self.accelerator.prepare(self.model,
                                                                                                     self.optim,
                                                                                                     self._get_batch,
                                                                                                     self.kbretriever.encoder
                                                                                                     )

    def setup_scheduler_and_optim(self):  # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨å’Œä¼˜åŒ–å™¨
        if self.sep_query_head:  # å¦‚æœåˆ†ç¦»æŸ¥è¯¢å¤´
            self.logger.info("Query head being fine tuned!")  # è®°å½•ä¿¡æ¯ï¼šæ­£åœ¨å¾®è°ƒæŸ¥è¯¢å¤´
            llm_q_params = self._get_params(self.model, self.sep_query_head, self.kb_token_layer_frequency)  # è·å–LLMæŸ¥è¯¢å‚æ•°
            scheduler, optim = setup_scheduler_and_optimizer(chain(self.kbretriever.encoder.parameters(), llm_q_params),
                                                             self.lr,
                                                             self.num_steps,
                                                             )
            self.logger.info("Optimizer recreated")  # è®°å½•ä¿¡æ¯ï¼šä¼˜åŒ–å™¨å·²é‡æ–°åˆ›å»º
        else:
            scheduler, optim = setup_scheduler_and_optimizer(self.kbretriever.encoder.parameters(), self.lr,
                                                             self.num_steps
                                                             )
            self.logger.info("Optimizer recreated")  # è®°å½•ä¿¡æ¯ï¼šä¼˜åŒ–å™¨å·²é‡æ–°åˆ›å»º
        return scheduler, optim  # è¿”å›è°ƒåº¦å™¨å’Œä¼˜åŒ–å™¨

    def train(self,
              console,
              training_set: List[Dict],  # è®­ç»ƒé›†
              batch_size,  # æ‰¹å¤„ç†å¤§å°
              grad_accum_steps: int,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
              outlier_num: int,  # å¼‚å¸¸å€¼æ•°é‡
              use_data_aug: bool = False,  # æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º
              multi_entities: bool = False,  # æ˜¯å¦ä½¿ç”¨å¤šé‡å®ä½“
              use_extended_qa: bool = False,  # æ˜¯å¦ä½¿ç”¨æ‰©å±•QA
              save_period: int = 2000,  # ä¿å­˜å‘¨æœŸ
              resumed_step: int = 0,  # æ¢å¤çš„æ­¥æ•°
              kb_config: KBLaMConfig = None,  # KBLAMé…ç½®
              ):
        train_losses = []  # ä¿å­˜è®­ç»ƒæŸå¤±
        start_step = resumed_step  # å¼€å§‹çš„æ­¥æ•°

        loss_fct = CrossEntropyLoss(reduction="none")  # åˆå§‹åŒ–äº¤å‰ç†µæŸå¤±å‡½æ•°

        # è®¡ç®—æ¯ä¸ªGPUçš„ç´¯ç§¯æ­¥æ•°
        num_processes = self.accelerator.num_processes  # è·å–è¿›ç¨‹æ•°é‡
        accum_steps_per_gpu = max(1, grad_accum_steps // num_processes)  # æ¯ä¸ªGPUçš„ç´¯ç§¯æ­¥æ•°
        effective_batch_size = batch_size * grad_accum_steps  # è®¡ç®—æœ‰æ•ˆæ‰¹å¤„ç†å¤§å°

        if self.accelerator.is_main_process:  # ä»…ä¸»è¿›ç¨‹è®°å½•
            self.logger.info(f"Training with {num_processes} GPUs")  # è®°å½•ä¿¡æ¯ï¼šä½¿ç”¨çš„GPUæ•°é‡
            self.logger.info(
                f"Total accumulation steps: {grad_accum_steps}, Steps per GPU: {accum_steps_per_gpu}")  # è®°å½•ä¿¡æ¯ï¼šæ€»ç´¯ç§¯æ­¥éª¤å’Œæ¯ä¸ªGPUæ­¥éª¤
            self.logger.info(f"Batch size: {batch_size}")  # è®°å½•ä¿¡æ¯ï¼šæ‰¹å¤„ç†å¤§å°
            self.logger.info(f"Effective batch size: {effective_batch_size}")  # è®°å½•ä¿¡æ¯ï¼šæœ‰æ•ˆæ‰¹å¤„ç†å¤§å°

        with create_custom_progress_bar(console=console,
                                        disable=not self.accelerator.is_main_process) as pbar:  # åˆ›å»ºè‡ªå®šä¹‰è¿›åº¦æ¡
            task = pbar.add_task("Training", total=self.num_steps, loss=100)  # æ·»åŠ ä»»åŠ¡åˆ°è¿›åº¦æ¡
            for step in range(start_step, self.num_steps, 1):  # è®­ç»ƒå¾ªç¯
                self.optim.zero_grad()  # æ¸…é›¶æ¢¯åº¦
                losses = []  # å­˜å‚¨æ¯ä¸ªæ­¥éª¤çš„æŸå¤±

                # è®¡ç®—æ­¤GPUåº”å¤„ç†çš„ç´¯ç§¯æ­¥éª¤
                process_rank = self.accelerator.process_index  # è·å–å½“å‰è¿›ç¨‹çš„ç´¢å¼•
                start_accum_step = process_rank * accum_steps_per_gpu  # è®¡ç®—å¼€å§‹çš„ç´¯ç§¯æ­¥éª¤
                end_accum_step = min(start_accum_step + accum_steps_per_gpu, grad_accum_steps)  # è®¡ç®—ç»“æŸçš„ç´¯ç§¯æ­¥éª¤

                # ç´¯ç§¯æ¢¯åº¦
                for a_step in range(start_accum_step, end_accum_step):  # éå†æ‰€æœ‰ç´¯ç§¯æ­¥éª¤
                    step_config = get_step_config(a_step,
                                                  grad_accum_steps,
                                                  use_data_aug,
                                                  outlier_num,
                                                  multi_entities,
                                                  use_extended_qa,
                                                  )  # è·å–æ­¤æ­¥çš„é…ç½®
                    input_ids, attention_masks, labels, batch_indices = self._get_batch(training_set,  # è·å–batch
                                                                                        self.tokenizer,
                                                                                        self.device,
                                                                                        B=batch_size,
                                                                                        random_sample=True,
                                                                                        **step_config,
                                                                                        )

                    if a_step == 0 and step % 10 == 0:  # æ¯10æ­¥è®°å½•ä¸€æ¬¡è¾“å…¥IDå½¢çŠ¶
                        self.logger.info(f"INPUT IDs SHAPE: {input_ids.shape}")

                    if self.max_seq_len is not None:  # å¦‚æœæœ€å¤§åºåˆ—é•¿åº¦ä¸ä¸ºç©º
                        input_ids = input_ids[:, : self.max_seq_len]  # æˆªå–è¾“å…¥ID
                        attention_masks = attention_masks[:, : self.max_seq_len]  # æˆªå–æ³¨æ„åŠ›æ©ç 
                        labels = labels[:, : self.max_seq_len]  # æˆªå–æ ‡ç­¾
                        if a_step == 0 and step % 10 == 0:  # æ¯10æ­¥è®°å½•ä¸€æ¬¡æˆªå–åçš„è¾“å…¥IDå½¢çŠ¶
                            self.logger.info(f"TRUNCATED INPUT IDs SHAPE: {input_ids.shape}")

                    kb_embedding = self.kbretriever.get_key_embeddings(batch_indices, len(input_ids), step,
                                                                       self.kb_size)  # è·å–çŸ¥è¯†åº“åµŒå…¥
                    out = self.model(input_ids=input_ids,  # ä½¿ç”¨æ¨¡å‹è®¡ç®—è¾“å‡º
                                     attention_mask=attention_masks,
                                     kb_kvs=kb_embedding,
                                     output_attentions=True,
                                     kb_config=kb_config,
                                     )
                    logits = out["logits"]  # æå–logitsè¾“å‡º

                    # æ˜¾ç¤ºçœŸå®æ ‡ç­¾å’Œæ¨¡å‹é¢„æµ‹ä»¥å¿«é€Ÿæ£€æŸ¥æ¨¡å‹
                    if a_step == 0 and step % 10 == 0:
                        batch_index = 0  # é€‰æ‹©æ‰¹å¤„ç†ä¸­çš„ä¸€ä¸ªç¤ºä¾‹
                        max_logits = logits.argmax(axis=2)  # è·å–æ¨¡å‹çš„é¢„æµ‹
                        decoded_pred = self.tokenizer.decode(max_logits[batch_index, :-1])  # è§£ç é¢„æµ‹
                        sel_labels = labels[batch_index, :]  # è·å–æ ‡ç­¾
                        sel_labels = sel_labels[sel_labels >= 0]  # ç§»é™¤å¡«å……æ ‡è®°-100
                        decoded_gt = self.tokenizer.decode(sel_labels)  # è§£ç çœŸå®æ ‡ç­¾
                        self.logger.info(f"KB SHAPE: {kb_embedding[0].shape}")  # è®°å½•çŸ¥è¯†åº“å½¢çŠ¶
                        self.logger.info(f"GT: {decoded_gt}")  # è®°å½•çœŸå®æ ‡ç­¾
                        self.logger.info(f"PRED: {decoded_pred}")  # è®°å½•é¢„æµ‹
                        wandb.log({"kbsize": kb_embedding[0].shape})
                        shift_logits = logits[..., :-1, :].contiguous()  # ç§»ä½logitsä»¥å¯¹åº”æ ‡ç­¾
                        shift_labels = labels[..., 1:].contiguous()  # ç§»ä½æ ‡ç­¾
                        weights = (shift_labels > 0).sum(-1, keepdim=True).expand(-1, shift_labels.shape[
                            1]).contiguous()  # è®¡ç®—æƒé‡
                        # æ‰å¹³åŒ–tokens
                        model_config = (self.model.config
                                        if not isinstance(self.model, DistributedDataParallel)
                                        else self.model.module.config
                                        )
                        shift_logits = shift_logits.view(-1, model_config.vocab_size)  # é‡å¡‘logits
                        shift_labels = shift_labels.view(-1)  # é‡å¡‘æ ‡ç­¾
                        weights = weights.view(-1)  # é‡å¡‘æƒé‡

                        shift_labels = shift_labels.to(shift_logits.device)  # ç§»åŠ¨æ ‡ç­¾åˆ°logitsè®¾å¤‡

                        # è®¡ç®—æŸå¤±
                        loss = (loss_fct(shift_logits, shift_labels) * weights.max() / weights
                                ).mean()  # ç¡®ä¿æ¯ä¸ªæ ·æœ¬çš„æƒé‡ç›¸ç­‰

                        self.accelerator.backward(loss)  # åå‘ä¼ æ’­
                        losses.append(loss.item())  # ä¿å­˜å½“å‰æŸå¤±

                    self.optim.step()  # ä¼˜åŒ–å™¨æ›´æ–°
                    if self.use_lr_decay:
                        self.scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡

                    # æ”¶é›†å¹¶å¹³å‡æ‰€æœ‰GPUçš„æŸå¤±ä»¥è¿›è¡ŒæŠ¥å‘Š
                    if losses:  # ä»…å½“è¯¥GPUå¤„ç†äº†ä»»ä½•æ‰¹æ¬¡æ—¶
                        local_loss = torch.tensor(np.mean(losses), device=self.device)  # è®¡ç®—æœ¬åœ°æŸå¤±
                    else:
                        local_loss = torch.tensor(0.0, device=self.device)  # å¦‚æœæ²¡æœ‰æŸå¤±ï¼Œåˆ™è®¾ä¸º0

                    # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„æŸå¤±
                    all_losses = self.accelerator.gather(local_loss)  # èšåˆæŸå¤±
                    valid_losses = all_losses[all_losses > 0]  # è¿‡æ»¤æ‰æœªå¤„ç†æ‰¹æ¬¡çš„é›¶
                    avg_loss = valid_losses.mean().item() if len(valid_losses) > 0 else 0.0  # è®¡ç®—å¹³å‡æŸå¤±

                    # ä»…ä»ä¸»è¿›ç¨‹è®°å½•
                    if self.accelerator.is_main_process:
                        self.logger.info(f"step: {step}, loss: {avg_loss}")  # è®°å½•è®­ç»ƒæ­¥æ•°å’ŒæŸå¤±
                        wandb.log({'train_loss': np.mean(losses)})  # è®°å½•åˆ°wandb
                        train_losses.append(avg_loss)  # ä¿å­˜è®­ç»ƒæŸå¤±
                        pbar.update(task, advance=1, loss=avg_loss)  # æ›´æ–°è¿›åº¦æ¡

                    # æ¯save_periodæ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹
                    if (step % save_period) == 0 and (step != start_step):
                        try:
                            # åœ¨åŒæ­¥ä¹‹å‰è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ
                            self.logger.info(
                                f"Is main process: {self.accelerator.is_main_process}, GPU memory before save: {torch.cuda.memory_allocated() / 1e9:.2f}GB / {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f}GB"
                            )

                            # å°è¯•é‡Šæ”¾å†…å­˜
                            torch.cuda.empty_cache()  # æ¸…é™¤ç¼“å­˜

                            # åœ¨ä¿å­˜ä¹‹å‰è¿›è¡ŒåŒæ­¥
                            self.accelerator.wait_for_everyone()

                            if self.accelerator.is_main_process:
                                self.logger.info("Saving checkpoint...")  # è®°å½•ä¿å­˜æ£€æŸ¥ç‚¹çš„ä¿¡æ¯
                                self.logger.info("Making dirs...")  # è®°å½•æ­£åœ¨åˆ›å»ºç›®å½•
                                # ä¿å­˜æ¨¡å‹ - ä½¿ç”¨é€‚å½“çš„ç›®å½•åˆ›å»º
                                model_ckpt_name = self.output_path / f"{self.llm_savename}_step_{step}"
                                model_ckpt_name.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºæ¨¡å‹æ£€æŸ¥ç‚¹ç›®å½•

                                # è¿˜åˆ›å»ºç¼–ç å™¨ç›®å½•
                                encoder_dir = self.output_path / f"{self.llm_savename}_step_{step}_encoder"
                                encoder_dir.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç¼–ç å™¨ä¿å­˜ç›®å½•

                                self.logger.info("Saving model...")  # è®°å½•ä¿å­˜æ¨¡å‹çš„ä¿¡æ¯
                                # è§£åŒ…å¹¶ä¿å­˜æ¨¡å‹
                                unwrapped_model = self.accelerator.unwrap_model(self.model)
                                unwrapped_model.save_pretrained(model_ckpt_name,
                                                                is_main_process=self.accelerator.is_main_process,
                                                                save_function=self.accelerator.save,
                                                                )

                                self.logger.info("Saving encoder...")  # è®°å½•ä¿å­˜ç¼–ç å™¨çš„ä¿¡æ¯
                                # ä»ä¸»è¿›ç¨‹ä¿å­˜ç¼–ç å™¨å’Œé…ç½®
                                encoder_ckpt_name = encoder_dir / "encoder.pt"
                                torch.save(self.kbretriever.encoder.state_dict(), encoder_ckpt_name)  # ä¿å­˜ç¼–ç å™¨çŠ¶æ€

                                self.logger.info("Saving config...")  # è®°å½•ä¿å­˜é…ç½®çš„ä¿¡æ¯
                                # æ˜ç¡®ä¿å­˜é…ç½®ä¸ºJSON
                                config_path = model_ckpt_name / "kb_config_explicit.json"
                                with open(config_path, 'w') as f:
                                    f.write(kb_config.to_json_string())  # ä¿å­˜KBé…ç½®ä¸ºJSONå­—ç¬¦ä¸²

                        except Exception as e:
                            self.logger.error(f"Error saving checkpoint: {e}")  # è®°å½•ä¿å­˜æ£€æŸ¥ç‚¹æ—¶çš„é”™è¯¯
                            self.logger.error(f"Error details: {str(e)}")  # è®°å½•é”™è¯¯è¯¦ç»†ä¿¡æ¯
                            raise e  # æŠ›å‡ºå¼‚å¸¸


if __name__ == "__main__":
    # æ—¥å¿—æ ¼å¼
    LOGFORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    LOGFORMAT_RICH = "%(message)s"

    # è®¾ç½®æ—¥å¿—é…ç½®
    custom_theme = Theme(styles={"info": "cyan",
                                 "warning": "yellow",
                                 "error": "bold red",
                                 "critical": "bold white on red"})
    # åˆ›å»ºå¯Œæ§åˆ¶å°
    console = Console(theme=custom_theme)

    # é…ç½®æ ¹è®°å½•å™¨ä¸ºè­¦å‘Šçº§åˆ«
    logging.basicConfig(level=logging.WARNING,
                        format=LOGFORMAT_RICH,
                        datefmt="[%X]",
                        handlers=[RichHandler(console=console, rich_tracebacks=True)])
    # è®¾ç½®NCCLè¶…æ—¶
    os.environ["NCCL_TIMEOUT"] = "1200000"

    # è·å–è®­ç»ƒè®°å½•å™¨
    logger = logging.getLogger("training")

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.get_arguments()

    if torch.cuda.is_available():
        device = torch.device("cuda")  # ä½¿ç”¨CUDAè®¾å¤‡

    if args.verbose:
        logger.setLevel(logging.DEBUG)  # å¦‚æœæ˜¯è¯¦ç»†æ¨¡å¼ï¼Œè®¾ç½®ä¸ºDEBUGçº§åˆ«
    else:
        logger.setLevel(logging.INFO)  # å¦åˆ™è®¾ç½®ä¸ºINFOçº§åˆ«

    print(vars(args))  # æ‰“å°å‚æ•°å­—å…¸

    dataset_name = args.train_dataset  # è·å–è®­ç»ƒæ•°æ®é›†åç§°
    seed = args.seed  # è·å–éšæœºç§å­
    N = args.num_train_sample  # è·å–æ•°æ®é›†å¤§å°
    batch_size = args.B  # è·å–æ‰¹å¤„ç†å¤§å°

    total_steps = args.total_step  # è·å–æ€»æ­¥æ•°
    encoder_spec = args.encoder_specification  # è·å–ç¼–ç å™¨è§„æ ¼
    key_embd_src = args.key_embed_source  # è·å–é”®åµŒå…¥æ¥æº
    use_data_aug = args.use_data_augment  # æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º
    use_lr_decay = args.use_learning_rate_decay  # æ˜¯å¦ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡
    use_cached_embd = args.use_cached_embed  # æ˜¯å¦ä½¿ç”¨ç¼“å­˜çš„åµŒå…¥
    dataset_dir = args.dataset_dir  # æ•°æ®é›†ç›®å½•
    model_dir_to_resume = args.model_dir_to_resume  # ç»§ç»­è®­ç»ƒçš„æ¨¡å‹ç›®å½•
    model_save_dir = args.model_save_dir  # æ¨¡å‹ä¿å­˜ç›®å½•
    sep_query_head = args.separate_query_head  # æ˜¯å¦åˆ†ç¦»æŸ¥è¯¢å¤´
    kb_size = args.kb_size  # çŸ¥è¯†åº“å¤§å°
    dynamic_kb_size = args.dynamic_kb_size  # åŠ¨æ€çŸ¥è¯†åº“å¤§å°
    max_seq_len = args.max_seq_len  # æœ€å¤§åºåˆ—é•¿åº¦

    if kb_size is not None and dynamic_kb_size is not None:
        raise ValueError("Can't specify kb_size and dynamic_kb_size. Use only one")  # ä¸èƒ½åŒæ—¶æŒ‡å®šä¸¤ä¸ªå‚æ•°

    kb_size = kb_size if kb_size is not None else dynamic_kb_size  # è®¾ç½®çŸ¥è¯†åº“å¤§å°

    gradient_accm_step = args.gradient_accumulation_step  # è·å–æ¢¯åº¦ç´¯ç§¯æ­¥æ•°

    length_invariance = args.length_invariance  # æ˜¯å¦é•¿çŸ­ä¸å˜
    outlier_num = args.outlier_num  # å¼‚å¸¸å€¼æ•°é‡
    multi_entities = args.multi_entity  # å¤šé‡å®ä½“æ•°é‡
    use_extended_qa = args.use_extended_qa  # æ˜¯å¦ä½¿ç”¨æ‰©å±•QA
    kb_token_layer_frequency = args.kb_token_layer_frequency  # KB tokenå±‚é¢‘ç‡
    llm_type = args.llm_type  # LLMç±»å‹
    hf_model_spec = args.hf_model_specification  # HuggingFaceæ¨¡å‹è§„æ ¼
    hf_token = args.hf_token  # HuggingFaceä»¤ç‰Œ

    torch.manual_seed(seed)  # è®¾ç½®PyTorchéšæœºç§å­
    np.random.seed(seed)  # è®¾ç½®NumPyéšæœºç§å­

    pathlib.Path(model_save_dir).mkdir(parents=True, exist_ok=True)  # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•

    if Accelerator().is_main_process:  # ä»…ä¸»è¿›ç¨‹åˆå§‹åŒ–wandb
        wandb.init(  # è®¾ç½®wandbé¡¹ç›®
            project="kb-llm",
            # è·Ÿè¸ªè¶…å‚æ•°å’Œè¿è¡Œå…ƒæ•°æ®
            config={
                "learning_rate": args.lr,  # å­¦ä¹ ç‡
                'sep_query_head': sep_query_head,  # æ˜¯å¦åˆ†ç¦»æŸ¥è¯¢å¤´
                'kb_size': kb_size,  # çŸ¥è¯†åº“å¤§å°
                'length_invariance': length_invariance,  # æ˜¯å¦é•¿çŸ­ä¸å˜
                'dataset': dataset_name,  # æ•°æ®é›†åç§°
                'outlier_num': outlier_num,  # å¼‚å¸¸å€¼æ•°é‡
                'multi_entities': multi_entities,  # å¤šé‡å®ä½“æ•°é‡
                'use_extended_qa': use_extended_qa,  # æ˜¯å¦ä½¿ç”¨æ‰©å±•QA
                'kb_token_layer_frequency': kb_token_layer_frequency,  # KB tokenå±‚é¢‘ç‡
                'gradient_accm_step': gradient_accm_step,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
                "encoder_spec": encoder_spec,  # ç¼–ç å™¨è§„æ ¼
                "max_seq_len": max_seq_len,  # æœ€å¤§åºåˆ—é•¿åº¦
            },
        )

    # å°è¯•é‡Šæ”¾å†…å­˜
    torch.cuda.empty_cache()

    if args.log_to_file:  # å¦‚æœéœ€è¦è®°å½•åˆ°æ–‡ä»¶
        formatter = logging.Formatter(LOGFORMAT)  # è®¾ç½®æ—¥å¿—æ ¼å¼åŒ–å™¨
        f_handler = logging.FileHandler(model_save_dir / "log.txt")  # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        f_handler.setFormatter(formatter)  # è®¾ç½®æ ¼å¼åŒ–å™¨
        logger.addHandler(f_handler)  # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨åˆ°è®°å½•å™¨

    logger.info(f"Running on {device}")  # è®°å½•å½“å‰è¿è¡Œè®¾å¤‡

    logger.info("ğŸš¨ Started training ğŸš¨")  # è®°å½•è®­ç»ƒå¼€å§‹
    logger.info(f"ğŸ’½ Saving to  {model_save_dir}ğŸ’½")  # è®°å½•æ¨¡å‹ä¿å­˜ç›®å½•
    if sep_query_head:
        os.environ["SEP_QUERY_HEAD"] = "TRUE"  # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥åˆ†ç¦»æŸ¥è¯¢å¤´
        logger.info("Having separate query head for KB!")  # è®°å½•ä¿¡æ¯ï¼šé‡‡ç”¨åˆ†ç¦»æŸ¥è¯¢å¤´

    if length_invariance:
        os.environ["LENGTH_INVARIANCE"] = "TRUE"  # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥è¡¨ç¤ºé•¿åº¦ä¸å˜
        logger.info("Having separate query head for KB!")  # è®°å½•ä¿¡æ¯ï¼šé‡‡ç”¨åˆ†ç¦»æŸ¥è¯¢å¤´

    os.environ["SCALE_FACTOR"] = ""  # è®¾ç½®ç¼©æ”¾å› å­ç¯å¢ƒå˜é‡

    if use_cached_embd:  # å¦‚æœä½¿ç”¨ç¼“å­˜çš„åµŒå…¥
        # æˆ‘ä»¬ä»ç£ç›˜åŠ è½½é¢„è®¡ç®—ç‰ˆæœ¬ï¼Œå¿«é€ŸåŒ–å¤„ç†
        logger.info(f"Using pre-computed {encoder_spec} embedding")  # è®°å½•ä¿¡æ¯ï¼šä½¿ç”¨é¢„è®¡ç®—åµŒå…¥
        key_embds, value_embds = load_cached_embeddings(encoder_spec, dataset_dir, dataset_name,
                                                        key_embd_src)  # åŠ è½½ç¼“å­˜çš„åµŒå…¥

    prefix_string = get_prefix_str(args)  # è·å–å®éªŒå‰ç¼€å­—ç¬¦ä¸²
    logger.info(f"Experiment prefix {get_prefix_str(args)}")  # è®°å½•å®éªŒå‰ç¼€

    if use_extended_qa:  # å¦‚æœä½¿ç”¨æ‰©å±•QA
        dataset = json.load(
            open(os.path.join(dataset_dir, f"{dataset_name}_augmented.json")))  # åŠ è½½æ‰©å±•æ•°æ®é›†
    else:
        dataset = json.load(open(os.path.join(dataset_dir, f"{dataset_name}.json")))  # åŠ è½½å¸¸è§„æ•°æ®é›†

    training_set = dataset[:N]  # åˆ’åˆ†å‡ºè®­ç»ƒé›†

    # è®¾ç½®LLMæ¨¡å‹
    llm_model_spec = model_dir_to_resume if model_dir_to_resume else hf_model_spec  # é€‰æ‹©æ¢å¤æ¨¡å‹æˆ–HuggingFaceæ¨¡å‹è§„æ ¼

    resumed_step = 0 if not model_dir_to_resume else int(model_dir_to_resume.split("_")[-1])  # è·å–æ¢å¤çš„æ­¥éª¤

    if llm_model_spec is None:
        raise ValueError(
            "Either supply model_dir_to_resume or hf_model_spec")  # å¿…é¡»æä¾›æ¨¡å‹ç›®å½•æˆ–HuggingFaceæ¨¡å‹è§„æ ¼

    if hf_token is None and args.llm_type == "llama3":  # å¦‚æœä½¿ç”¨Llama3æ¨¡å‹ä¸”æœªæä¾›ä»¤ç‰Œ
        raise ValueError(
            "Please supply HuggingFace token(hf_token) when loading model Llama weights from HuggingFace")  # æŠ›å‡ºé”™è¯¯ï¼Œéœ€æä¾›ä»¤ç‰Œ

    # Tokenizeræ¥è‡ªåŸºç¡€æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(hf_model_spec,
                                              trust_remote_code=True,
                                              token=hf_token if hf_token is not None and args.llm_type == "llama3" else None,
                                              )
    tokenizer.pad_token = tokenizer.eos_token  # è®¾ç½®å¡«å……æ ‡è®°ä¸ºç»“æŸæ ‡è®°

    if args.llm_type == "llama3":  # å¦‚æœé€‰æ‹©Llama3
        model = KblamLlamaForCausalLM.from_pretrained(llm_model_spec,  # ä»HuggingFaceåŠ è½½Llamaæ¨¡å‹
                                                      device_map=device,
                                                      torch_dtype=torch.bfloat16,
                                                      trust_remote_code=True,
                                                      token=hf_token,
                                                      )
    elif args.llm_type == "phi3":  # å¦‚æœé€‰æ‹©Phi3
        model = KBLaMPhi3ForCausalLM.from_pretrained(llm_model_spec,  # ä»HuggingFaceåŠ è½½Phi3æ¨¡å‹
                                                     device_map=device,
                                                     torch_dtype="auto",
                                                     trust_remote_code=True,
                                                     )
    else:
        raise ValueError(f"LLM type {args.llm_type} not recognised")  # æŠ›å‡ºé”™è¯¯ï¼Œæœªè¯†åˆ«çš„æ¨¡å‹ç±»å‹

    logger.info(model.config)  # è®°å½•æ¨¡å‹é…ç½®

    model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    # å†»ç»“æ¨¡å‹
    for _, param in model.named_parameters():  # éå†æ¨¡å‹å‚æ•°
        param.requires_grad = False  # ç¦ç”¨æ¢¯åº¦æ›´æ–°

    # è®¾ç½®ç¼–ç å™¨
    encoder = KBEncoder(encoder_name=encoder_spec,  # åˆ›å»ºçŸ¥è¯†åº“ç¼–ç å™¨
                        projector_type="linear",
                        endpoint_url="",
                        out_dim=model.config.hidden_size  # è®¾ç½®è¾“å‡ºç»´åº¦
                                * (model.config.num_hidden_layers // kb_token_layer_frequency + 1),
                        # è®¾ç½®è¾“å‡ºç»´åº¦
                        frozen_base_model=True,  # å†»ç»“åŸºç¡€æ¨¡å‹
                        device=device,  # è®¾ç½®è®¾å¤‡
                        )

    if model_dir_to_resume:  # å¦‚æœæä¾›äº†æ¢å¤ç›®å½•
        encoder.load_state_dict(torch.load(os.path.join(model_dir_to_resume, "encoder.pt")))  # åŠ è½½ç¼–ç å™¨çŠ¶æ€
        kb_config = KBLaMConfig.from_pretrained(
            os.path.join(model_dir_to_resume, "kb_config.json"))  # åŠ è½½KBé…ç½®
    else:
        kb_config = KBLaMConfig(sep_query_head=sep_query_head,  # åˆ›å»ºæ–°çš„KBé…ç½®
                                kb_layer_frequency=kb_token_layer_frequency,
                                )

    encoder.train()  # è®¾ç½®ç¼–ç å™¨ä¸ºè®­ç»ƒæ¨¡å¼

    kbretriever = KBRetriever(encoder,  # åˆ›å»ºKBRetrieverå¯¹è±¡
                              training_set,
                              key_embds=key_embds,  # ç”¨åŠ è½½çš„é”®åµŒå…¥
                              value_embds=value_embds,  # ç”¨åŠ è½½çš„å€¼åµŒå…¥
                              )

    logger.info("Model ready ğŸš€")  # è®°å½•æ¨¡å‹å‡†å¤‡å°±ç»ªçš„ä¿¡æ¯

    # å¼€å§‹è®­ç»ƒ
    llm_ckpt_name = f"{prefix_string}KeyFrom{key_embd_src}_{encoder_spec}_{dataset_name}_{llm_type}"  # åˆ›å»ºcheckpointåç§°

    trainer = Trainer(model,  # åˆ›å»ºTrainerå¯¹è±¡
                      kbretriever,
                      tokenizer,
                      kb_token_layer_frequency,
                      total_steps,
                      args.lr,
                      device,
                      use_lr_decay,
                      kb_size,  # ä¼ é€’çŸ¥è¯†åº“å¤§å°
                      llm_ckpt_name,
                      model_save_dir,
                      sep_query_head=sep_query_head,
                      max_seq_len=max_seq_len,
                      )

    logger.info(f"Number of trainable parameters: {get_parameter_count(encoder):,}")  # è®°å½•å¯è®­ç»ƒå‚æ•°çš„æ•°é‡

    trainer.train(training_set,  # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
                  batch_size,
                  gradient_accm_step,
                  outlier_num,
                  use_data_aug=use_data_aug,
                  multi_entities=multi_entities,
                  use_extended_qa=use_extended_qa,
                  save_period=3000,  # è®¾ç½®ä¿å­˜å‘¨æœŸ
                  resumed_step=resumed_step,  # è®¾ç½®æ¢å¤æ­¥æ•°
                  kb_config=kb_config,  # è®¾ç½®KBé…ç½®
                  )
